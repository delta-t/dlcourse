{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 691.126734\n",
      "Epoch 1, loss: 717.019993\n",
      "Epoch 2, loss: 742.517739\n",
      "Epoch 3, loss: 767.434835\n",
      "Epoch 4, loss: 781.312402\n",
      "Epoch 5, loss: 751.616065\n",
      "Epoch 6, loss: 741.137412\n",
      "Epoch 7, loss: 799.083768\n",
      "Epoch 8, loss: 912.039534\n",
      "Epoch 9, loss: 947.064208\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0cd1e53748>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.146\n",
      "Epoch 0, loss: 713.445531\n",
      "Epoch 1, loss: 719.195132\n",
      "Epoch 2, loss: 713.910978\n",
      "Epoch 3, loss: 727.410103\n",
      "Epoch 4, loss: 749.326307\n",
      "Epoch 5, loss: 837.423965\n",
      "Epoch 6, loss: 1018.506335\n",
      "Epoch 7, loss: 1390.977540\n",
      "Epoch 8, loss: 1067.984933\n",
      "Epoch 9, loss: 1019.505285\n",
      "Epoch 10, loss: 1001.378783\n",
      "Epoch 11, loss: 937.158608\n",
      "Epoch 12, loss: 914.534530\n",
      "Epoch 13, loss: 871.930016\n",
      "Epoch 14, loss: 790.971990\n",
      "Epoch 15, loss: 797.761611\n",
      "Epoch 16, loss: 842.667193\n",
      "Epoch 17, loss: 909.083294\n",
      "Epoch 18, loss: 977.683611\n",
      "Epoch 19, loss: 732.855889\n",
      "Epoch 20, loss: 790.471729\n",
      "Epoch 21, loss: 796.253121\n",
      "Epoch 22, loss: 892.691976\n",
      "Epoch 23, loss: 1015.739307\n",
      "Epoch 24, loss: 791.692767\n",
      "Epoch 25, loss: 798.720621\n",
      "Epoch 26, loss: 943.640671\n",
      "Epoch 27, loss: 901.906725\n",
      "Epoch 28, loss: 874.624438\n",
      "Epoch 29, loss: 854.149489\n",
      "Epoch 30, loss: 866.985139\n",
      "Epoch 31, loss: 943.287905\n",
      "Epoch 32, loss: 1012.480541\n",
      "Epoch 33, loss: 970.016200\n",
      "Epoch 34, loss: 896.244224\n",
      "Epoch 35, loss: 977.069521\n",
      "Epoch 36, loss: 847.569330\n",
      "Epoch 37, loss: 844.771736\n",
      "Epoch 38, loss: 885.638455\n",
      "Epoch 39, loss: 794.408069\n",
      "Epoch 40, loss: 790.879555\n",
      "Epoch 41, loss: 852.639806\n",
      "Epoch 42, loss: 804.301091\n",
      "Epoch 43, loss: 766.641535\n",
      "Epoch 44, loss: 827.214591\n",
      "Epoch 45, loss: 940.889047\n",
      "Epoch 46, loss: 833.994715\n",
      "Epoch 47, loss: 940.434474\n",
      "Epoch 48, loss: 819.312821\n",
      "Epoch 49, loss: 706.124100\n",
      "Epoch 50, loss: 717.560564\n",
      "Epoch 51, loss: 728.669721\n",
      "Epoch 52, loss: 733.045166\n",
      "Epoch 53, loss: 726.968780\n",
      "Epoch 54, loss: 725.979543\n",
      "Epoch 55, loss: 761.878731\n",
      "Epoch 56, loss: 856.263099\n",
      "Epoch 57, loss: 941.635524\n",
      "Epoch 58, loss: 901.816232\n",
      "Epoch 59, loss: 992.827709\n",
      "Epoch 60, loss: 949.665032\n",
      "Epoch 61, loss: 917.335066\n",
      "Epoch 62, loss: 789.808269\n",
      "Epoch 63, loss: 888.269306\n",
      "Epoch 64, loss: 924.341352\n",
      "Epoch 65, loss: 937.101354\n",
      "Epoch 66, loss: 992.200459\n",
      "Epoch 67, loss: 875.290796\n",
      "Epoch 68, loss: 838.560468\n",
      "Epoch 69, loss: 767.383133\n",
      "Epoch 70, loss: 863.819949\n",
      "Epoch 71, loss: 889.063082\n",
      "Epoch 72, loss: 834.075928\n",
      "Epoch 73, loss: 903.442391\n",
      "Epoch 74, loss: 886.298814\n",
      "Epoch 75, loss: 771.308259\n",
      "Epoch 76, loss: 850.282288\n",
      "Epoch 77, loss: 870.301040\n",
      "Epoch 78, loss: 917.269375\n",
      "Epoch 79, loss: 903.031287\n",
      "Epoch 80, loss: 951.178964\n",
      "Epoch 81, loss: 872.532783\n",
      "Epoch 82, loss: 921.307667\n",
      "Epoch 83, loss: 964.397168\n",
      "Epoch 84, loss: 937.869507\n",
      "Epoch 85, loss: 891.832059\n",
      "Epoch 86, loss: 905.351204\n",
      "Epoch 87, loss: 952.093536\n",
      "Epoch 88, loss: 908.536641\n",
      "Epoch 89, loss: 939.026947\n",
      "Epoch 90, loss: 875.377630\n",
      "Epoch 91, loss: 912.054670\n",
      "Epoch 92, loss: 976.455185\n",
      "Epoch 93, loss: 917.088490\n",
      "Epoch 94, loss: 777.005799\n",
      "Epoch 95, loss: 779.171363\n",
      "Epoch 96, loss: 768.005378\n",
      "Epoch 97, loss: 806.134565\n",
      "Epoch 98, loss: 737.295711\n",
      "Epoch 99, loss: 748.748243\n",
      "Accuracy after training for 100 epochs:  0.14\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 690.501273\n",
      "Epoch 1, loss: 718.707597\n",
      "Epoch 2, loss: 741.726298\n",
      "Epoch 3, loss: 735.583084\n",
      "Epoch 4, loss: 762.983747\n",
      "Epoch 5, loss: 749.831946\n",
      "Epoch 6, loss: 749.628129\n",
      "Epoch 7, loss: 793.681897\n",
      "Epoch 8, loss: 873.016222\n",
      "Epoch 9, loss: 889.714694\n",
      "Epoch 10, loss: 913.934917\n",
      "Epoch 11, loss: 865.572821\n",
      "Epoch 12, loss: 831.158274\n",
      "Epoch 13, loss: 881.279711\n",
      "Epoch 14, loss: 1004.214847\n",
      "Epoch 15, loss: 894.025236\n",
      "Epoch 16, loss: 1009.999662\n",
      "Epoch 17, loss: 1004.971168\n",
      "Epoch 18, loss: 1039.482164\n",
      "Epoch 19, loss: 977.419734\n",
      "Epoch 20, loss: 936.055854\n",
      "Epoch 21, loss: 965.713103\n",
      "Epoch 22, loss: 947.438042\n",
      "Epoch 23, loss: 729.064624\n",
      "Epoch 24, loss: 700.478095\n",
      "Epoch 25, loss: 713.552574\n",
      "Epoch 26, loss: 746.882101\n",
      "Epoch 27, loss: 853.458697\n",
      "Epoch 28, loss: 1044.272292\n",
      "Epoch 29, loss: 952.920357\n",
      "Epoch 30, loss: 766.052627\n",
      "Epoch 31, loss: 793.717350\n",
      "Epoch 32, loss: 857.629244\n",
      "Epoch 33, loss: 917.660322\n",
      "Epoch 34, loss: 903.328182\n",
      "Epoch 35, loss: 897.649874\n",
      "Epoch 36, loss: 764.262190\n",
      "Epoch 37, loss: 752.819287\n",
      "Epoch 38, loss: 723.832723\n",
      "Epoch 39, loss: 734.676765\n",
      "Epoch 40, loss: 756.608378\n",
      "Epoch 41, loss: 797.170332\n",
      "Epoch 42, loss: 822.000697\n",
      "Epoch 43, loss: 902.700480\n",
      "Epoch 44, loss: 1064.983842\n",
      "Epoch 45, loss: 1113.092534\n",
      "Epoch 46, loss: 950.122685\n",
      "Epoch 47, loss: 992.040749\n",
      "Epoch 48, loss: 945.563628\n",
      "Epoch 49, loss: 843.536711\n",
      "Epoch 50, loss: 768.958044\n",
      "Epoch 51, loss: 716.833831\n",
      "Epoch 52, loss: 694.712858\n",
      "Epoch 53, loss: 690.599062\n",
      "Epoch 54, loss: 705.169046\n",
      "Epoch 55, loss: 760.859779\n",
      "Epoch 56, loss: 846.487081\n",
      "Epoch 57, loss: 904.563395\n",
      "Epoch 58, loss: 868.791008\n",
      "Epoch 59, loss: 837.209110\n",
      "Epoch 60, loss: 966.717516\n",
      "Epoch 61, loss: 964.198111\n",
      "Epoch 62, loss: 988.315586\n",
      "Epoch 63, loss: 947.250204\n",
      "Epoch 64, loss: 1034.825986\n",
      "Epoch 65, loss: 1125.206485\n",
      "Epoch 66, loss: 1051.430569\n",
      "Epoch 67, loss: 845.786391\n",
      "Epoch 68, loss: 892.303173\n",
      "Epoch 69, loss: 878.163513\n",
      "Epoch 70, loss: 920.734915\n",
      "Epoch 71, loss: 963.045748\n",
      "Epoch 72, loss: 925.858201\n",
      "Epoch 73, loss: 889.734131\n",
      "Epoch 74, loss: 958.101302\n",
      "Epoch 75, loss: 1048.328810\n",
      "Epoch 76, loss: 1194.172902\n",
      "Epoch 77, loss: 1087.574563\n",
      "Epoch 78, loss: 1029.078988\n",
      "Epoch 79, loss: 928.015251\n",
      "Epoch 80, loss: 972.898839\n",
      "Epoch 81, loss: 987.222408\n",
      "Epoch 82, loss: 755.207144\n",
      "Epoch 83, loss: 721.142240\n",
      "Epoch 84, loss: 726.320781\n",
      "Epoch 85, loss: 740.078836\n",
      "Epoch 86, loss: 726.585734\n",
      "Epoch 87, loss: 743.789643\n",
      "Epoch 88, loss: 785.388134\n",
      "Epoch 89, loss: 744.505386\n",
      "Epoch 90, loss: 760.437270\n",
      "Epoch 91, loss: 764.122729\n",
      "Epoch 92, loss: 845.519708\n",
      "Epoch 93, loss: 839.953778\n",
      "Epoch 94, loss: 796.844852\n",
      "Epoch 95, loss: 741.338786\n",
      "Epoch 96, loss: 700.259610\n",
      "Epoch 97, loss: 697.338089\n",
      "Epoch 98, loss: 745.397929\n",
      "Epoch 99, loss: 746.591472\n",
      "Epoch 100, loss: 738.840925\n",
      "Epoch 101, loss: 824.817618\n",
      "Epoch 102, loss: 822.584986\n",
      "Epoch 103, loss: 1016.518048\n",
      "Epoch 104, loss: 832.718749\n",
      "Epoch 105, loss: 788.286369\n",
      "Epoch 106, loss: 687.201457\n",
      "Epoch 107, loss: 714.907264\n",
      "Epoch 108, loss: 814.719790\n",
      "Epoch 109, loss: 861.578126\n",
      "Epoch 110, loss: 883.619616\n",
      "Epoch 111, loss: 1026.459928\n",
      "Epoch 112, loss: 1013.157342\n",
      "Epoch 113, loss: 820.350211\n",
      "Epoch 114, loss: 831.908385\n",
      "Epoch 115, loss: 798.443761\n",
      "Epoch 116, loss: 891.011491\n",
      "Epoch 117, loss: 1017.657117\n",
      "Epoch 118, loss: 937.387892\n",
      "Epoch 119, loss: 891.773079\n",
      "Epoch 120, loss: 834.397095\n",
      "Epoch 121, loss: 892.907327\n",
      "Epoch 122, loss: 931.010378\n",
      "Epoch 123, loss: 937.591695\n",
      "Epoch 124, loss: 944.544005\n",
      "Epoch 125, loss: 925.575071\n",
      "Epoch 126, loss: 939.857951\n",
      "Epoch 127, loss: 937.572234\n",
      "Epoch 128, loss: 784.741718\n",
      "Epoch 129, loss: 802.450097\n",
      "Epoch 130, loss: 867.518951\n",
      "Epoch 131, loss: 908.424537\n",
      "Epoch 132, loss: 857.544280\n",
      "Epoch 133, loss: 773.578282\n",
      "Epoch 134, loss: 895.929296\n",
      "Epoch 135, loss: 923.749739\n",
      "Epoch 136, loss: 876.347667\n",
      "Epoch 137, loss: 879.868777\n",
      "Epoch 138, loss: 921.103071\n",
      "Epoch 139, loss: 836.745352\n",
      "Epoch 140, loss: 796.393910\n",
      "Epoch 141, loss: 888.524683\n",
      "Epoch 142, loss: 989.073213\n",
      "Epoch 143, loss: 713.391384\n",
      "Epoch 144, loss: 781.589895\n",
      "Epoch 145, loss: 787.711895\n",
      "Epoch 146, loss: 921.192312\n",
      "Epoch 147, loss: 933.839299\n",
      "Epoch 148, loss: 845.734341\n",
      "Epoch 149, loss: 827.892693\n",
      "Epoch 150, loss: 951.569267\n",
      "Epoch 151, loss: 1099.376425\n",
      "Epoch 152, loss: 977.576550\n",
      "Epoch 153, loss: 913.078661\n",
      "Epoch 154, loss: 781.264585\n",
      "Epoch 155, loss: 861.010981\n",
      "Epoch 156, loss: 957.001511\n",
      "Epoch 157, loss: 1017.545926\n",
      "Epoch 158, loss: 1064.657248\n",
      "Epoch 159, loss: 939.606944\n",
      "Epoch 160, loss: 860.094105\n",
      "Epoch 161, loss: 832.874396\n",
      "Epoch 162, loss: 802.689549\n",
      "Epoch 163, loss: 819.184099\n",
      "Epoch 164, loss: 812.141983\n",
      "Epoch 165, loss: 832.939024\n",
      "Epoch 166, loss: 726.269645\n",
      "Epoch 167, loss: 712.288139\n",
      "Epoch 168, loss: 793.938270\n",
      "Epoch 169, loss: 846.002544\n",
      "Epoch 170, loss: 807.445206\n",
      "Epoch 171, loss: 875.037513\n",
      "Epoch 172, loss: 824.279665\n",
      "Epoch 173, loss: 866.218733\n",
      "Epoch 174, loss: 976.871888\n",
      "Epoch 175, loss: 947.450704\n",
      "Epoch 176, loss: 881.673140\n",
      "Epoch 177, loss: 823.178407\n",
      "Epoch 178, loss: 873.079173\n",
      "Epoch 179, loss: 859.146816\n",
      "Epoch 180, loss: 897.560551\n",
      "Epoch 181, loss: 862.548659\n",
      "Epoch 182, loss: 851.192027\n",
      "Epoch 183, loss: 893.993114\n",
      "Epoch 184, loss: 799.312612\n",
      "Epoch 185, loss: 712.905958\n",
      "Epoch 186, loss: 740.977693\n",
      "Epoch 187, loss: 732.028485\n",
      "Epoch 188, loss: 742.931933\n",
      "Epoch 189, loss: 766.632439\n",
      "Epoch 190, loss: 781.961566\n",
      "Epoch 191, loss: 799.948777\n",
      "Epoch 192, loss: 800.191753\n",
      "Epoch 193, loss: 904.321912\n",
      "Epoch 194, loss: 868.775294\n",
      "Epoch 195, loss: 934.745716\n",
      "Epoch 196, loss: 886.867424\n",
      "Epoch 197, loss: 835.259149\n",
      "Epoch 198, loss: 793.533854\n",
      "Epoch 199, loss: 739.691226\n",
      "Epoch 0, loss: 726.818723\n",
      "Epoch 1, loss: 739.237339\n",
      "Epoch 2, loss: 770.158991\n",
      "Epoch 3, loss: 872.082248\n",
      "Epoch 4, loss: 943.236006\n",
      "Epoch 5, loss: 953.585139\n",
      "Epoch 6, loss: 808.015787\n",
      "Epoch 7, loss: 906.234013\n",
      "Epoch 8, loss: 878.774489\n",
      "Epoch 9, loss: 794.587753\n",
      "Epoch 10, loss: 832.936539\n",
      "Epoch 11, loss: 957.808572\n",
      "Epoch 12, loss: 807.707621\n",
      "Epoch 13, loss: 864.749047\n",
      "Epoch 14, loss: 936.880468\n",
      "Epoch 15, loss: 854.827031\n",
      "Epoch 16, loss: 779.662134\n",
      "Epoch 17, loss: 825.167725\n",
      "Epoch 18, loss: 886.131912\n",
      "Epoch 19, loss: 980.592725\n",
      "Epoch 20, loss: 898.382160\n",
      "Epoch 21, loss: 932.821313\n",
      "Epoch 22, loss: 913.568349\n",
      "Epoch 23, loss: 977.544698\n",
      "Epoch 24, loss: 852.907042\n",
      "Epoch 25, loss: 907.794109\n",
      "Epoch 26, loss: 913.615956\n",
      "Epoch 27, loss: 821.431010\n",
      "Epoch 28, loss: 787.557084\n",
      "Epoch 29, loss: 709.592719\n",
      "Epoch 30, loss: 713.646474\n",
      "Epoch 31, loss: 759.302219\n",
      "Epoch 32, loss: 691.092159\n",
      "Epoch 33, loss: 763.047970\n",
      "Epoch 34, loss: 735.049576\n",
      "Epoch 35, loss: 819.011892\n",
      "Epoch 36, loss: 791.504670\n",
      "Epoch 37, loss: 864.948511\n",
      "Epoch 38, loss: 980.914146\n",
      "Epoch 39, loss: 1062.290426\n",
      "Epoch 40, loss: 902.518981\n",
      "Epoch 41, loss: 906.670541\n",
      "Epoch 42, loss: 889.587985\n",
      "Epoch 43, loss: 840.656887\n",
      "Epoch 44, loss: 851.872179\n",
      "Epoch 45, loss: 890.504494\n",
      "Epoch 46, loss: 927.817285\n",
      "Epoch 47, loss: 994.197715\n",
      "Epoch 48, loss: 973.094567\n",
      "Epoch 49, loss: 821.380888\n",
      "Epoch 50, loss: 839.182903\n",
      "Epoch 51, loss: 915.381616\n",
      "Epoch 52, loss: 858.664841\n",
      "Epoch 53, loss: 864.828933\n",
      "Epoch 54, loss: 794.533192\n",
      "Epoch 55, loss: 896.622356\n",
      "Epoch 56, loss: 1092.691473\n",
      "Epoch 57, loss: 953.245559\n",
      "Epoch 58, loss: 801.401648\n",
      "Epoch 59, loss: 689.037904\n",
      "Epoch 60, loss: 678.929734\n",
      "Epoch 61, loss: 737.341061\n",
      "Epoch 62, loss: 747.557600\n",
      "Epoch 63, loss: 779.650655\n",
      "Epoch 64, loss: 815.328375\n",
      "Epoch 65, loss: 876.384363\n",
      "Epoch 66, loss: 900.952748\n",
      "Epoch 67, loss: 824.963547\n",
      "Epoch 68, loss: 837.801836\n",
      "Epoch 69, loss: 900.921324\n",
      "Epoch 70, loss: 831.496770\n",
      "Epoch 71, loss: 760.846909\n",
      "Epoch 72, loss: 794.361317\n",
      "Epoch 73, loss: 857.941337\n",
      "Epoch 74, loss: 842.193251\n",
      "Epoch 75, loss: 860.096844\n",
      "Epoch 76, loss: 704.662120\n",
      "Epoch 77, loss: 700.084462\n",
      "Epoch 78, loss: 713.211149\n",
      "Epoch 79, loss: 780.404979\n",
      "Epoch 80, loss: 783.758834\n",
      "Epoch 81, loss: 881.409493\n",
      "Epoch 82, loss: 876.466682\n",
      "Epoch 83, loss: 1027.125842\n",
      "Epoch 84, loss: 956.266100\n",
      "Epoch 85, loss: 919.446343\n",
      "Epoch 86, loss: 977.925397\n",
      "Epoch 87, loss: 823.206727\n",
      "Epoch 88, loss: 810.897253\n",
      "Epoch 89, loss: 928.206466\n",
      "Epoch 90, loss: 833.328992\n",
      "Epoch 91, loss: 844.600208\n",
      "Epoch 92, loss: 754.971564\n",
      "Epoch 93, loss: 759.759076\n",
      "Epoch 94, loss: 768.116102\n",
      "Epoch 95, loss: 826.582273\n",
      "Epoch 96, loss: 833.205945\n",
      "Epoch 97, loss: 782.988426\n",
      "Epoch 98, loss: 862.838127\n",
      "Epoch 99, loss: 801.754012\n",
      "Epoch 100, loss: 737.682283\n",
      "Epoch 101, loss: 750.723908\n",
      "Epoch 102, loss: 749.928933\n",
      "Epoch 103, loss: 749.968124\n",
      "Epoch 104, loss: 849.620652\n",
      "Epoch 105, loss: 838.925337\n",
      "Epoch 106, loss: 759.014888\n",
      "Epoch 107, loss: 782.346334\n",
      "Epoch 108, loss: 740.659695\n",
      "Epoch 109, loss: 732.786898\n",
      "Epoch 110, loss: 948.181325\n",
      "Epoch 111, loss: 826.482529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112, loss: 855.631110\n",
      "Epoch 113, loss: 860.118754\n",
      "Epoch 114, loss: 734.169140\n",
      "Epoch 115, loss: 826.173123\n",
      "Epoch 116, loss: 816.309459\n",
      "Epoch 117, loss: 814.769060\n",
      "Epoch 118, loss: 849.054298\n",
      "Epoch 119, loss: 775.216932\n",
      "Epoch 120, loss: 712.807350\n",
      "Epoch 121, loss: 753.257931\n",
      "Epoch 122, loss: 742.103263\n",
      "Epoch 123, loss: 742.319054\n",
      "Epoch 124, loss: 857.124015\n",
      "Epoch 125, loss: 891.106511\n",
      "Epoch 126, loss: 989.302360\n",
      "Epoch 127, loss: 992.156376\n",
      "Epoch 128, loss: 957.972094\n",
      "Epoch 129, loss: 886.234440\n",
      "Epoch 130, loss: 768.004850\n",
      "Epoch 131, loss: 762.386805\n",
      "Epoch 132, loss: 709.378363\n",
      "Epoch 133, loss: 734.792872\n",
      "Epoch 134, loss: 746.182593\n",
      "Epoch 135, loss: 841.095575\n",
      "Epoch 136, loss: 986.446680\n",
      "Epoch 137, loss: 1147.348881\n",
      "Epoch 138, loss: 803.169453\n",
      "Epoch 139, loss: 806.215648\n",
      "Epoch 140, loss: 771.554619\n",
      "Epoch 141, loss: 906.306918\n",
      "Epoch 142, loss: 830.515647\n",
      "Epoch 143, loss: 745.874772\n",
      "Epoch 144, loss: 753.184280\n",
      "Epoch 145, loss: 774.483761\n",
      "Epoch 146, loss: 809.037958\n",
      "Epoch 147, loss: 755.988243\n",
      "Epoch 148, loss: 704.889485\n",
      "Epoch 149, loss: 789.417352\n",
      "Epoch 150, loss: 917.284817\n",
      "Epoch 151, loss: 1111.348024\n",
      "Epoch 152, loss: 851.062823\n",
      "Epoch 153, loss: 862.564416\n",
      "Epoch 154, loss: 879.656192\n",
      "Epoch 155, loss: 766.886454\n",
      "Epoch 156, loss: 798.155545\n",
      "Epoch 157, loss: 966.409078\n",
      "Epoch 158, loss: 877.927126\n",
      "Epoch 159, loss: 819.719094\n",
      "Epoch 160, loss: 949.164745\n",
      "Epoch 161, loss: 796.396099\n",
      "Epoch 162, loss: 800.141640\n",
      "Epoch 163, loss: 716.903893\n",
      "Epoch 164, loss: 786.757219\n",
      "Epoch 165, loss: 825.388293\n",
      "Epoch 166, loss: 823.713183\n",
      "Epoch 167, loss: 873.410158\n",
      "Epoch 168, loss: 1120.749348\n",
      "Epoch 169, loss: 911.750198\n",
      "Epoch 170, loss: 824.910692\n",
      "Epoch 171, loss: 766.970953\n",
      "Epoch 172, loss: 752.829863\n",
      "Epoch 173, loss: 716.162353\n",
      "Epoch 174, loss: 784.773389\n",
      "Epoch 175, loss: 877.973752\n",
      "Epoch 176, loss: 1115.938253\n",
      "Epoch 177, loss: 874.527605\n",
      "Epoch 178, loss: 874.963967\n",
      "Epoch 179, loss: 782.427651\n",
      "Epoch 180, loss: 721.556939\n",
      "Epoch 181, loss: 706.523175\n",
      "Epoch 182, loss: 721.378363\n",
      "Epoch 183, loss: 789.062274\n",
      "Epoch 184, loss: 882.168332\n",
      "Epoch 185, loss: 833.535684\n",
      "Epoch 186, loss: 862.583862\n",
      "Epoch 187, loss: 825.773607\n",
      "Epoch 188, loss: 905.144977\n",
      "Epoch 189, loss: 928.261222\n",
      "Epoch 190, loss: 752.016831\n",
      "Epoch 191, loss: 856.157286\n",
      "Epoch 192, loss: 903.085859\n",
      "Epoch 193, loss: 865.186503\n",
      "Epoch 194, loss: 819.790706\n",
      "Epoch 195, loss: 834.005813\n",
      "Epoch 196, loss: 801.238742\n",
      "Epoch 197, loss: 772.438295\n",
      "Epoch 198, loss: 779.574116\n",
      "Epoch 199, loss: 758.336598\n",
      "Epoch 0, loss: 718.641374\n",
      "Epoch 1, loss: 727.536655\n",
      "Epoch 2, loss: 781.844609\n",
      "Epoch 3, loss: 759.038293\n",
      "Epoch 4, loss: 835.548444\n",
      "Epoch 5, loss: 855.726791\n",
      "Epoch 6, loss: 806.617994\n",
      "Epoch 7, loss: 863.535530\n",
      "Epoch 8, loss: 888.438311\n",
      "Epoch 9, loss: 742.709000\n",
      "Epoch 10, loss: 799.986550\n",
      "Epoch 11, loss: 823.365652\n",
      "Epoch 12, loss: 862.512476\n",
      "Epoch 13, loss: 874.661881\n",
      "Epoch 14, loss: 785.758579\n",
      "Epoch 15, loss: 735.271235\n",
      "Epoch 16, loss: 747.610117\n",
      "Epoch 17, loss: 753.918215\n",
      "Epoch 18, loss: 793.280428\n",
      "Epoch 19, loss: 752.802774\n",
      "Epoch 20, loss: 892.813855\n",
      "Epoch 21, loss: 950.129480\n",
      "Epoch 22, loss: 758.596585\n",
      "Epoch 23, loss: 761.766423\n",
      "Epoch 24, loss: 754.548624\n",
      "Epoch 25, loss: 782.844403\n",
      "Epoch 26, loss: 768.254433\n",
      "Epoch 27, loss: 797.070193\n",
      "Epoch 28, loss: 813.030088\n",
      "Epoch 29, loss: 835.184043\n",
      "Epoch 30, loss: 855.695174\n",
      "Epoch 31, loss: 847.273355\n",
      "Epoch 32, loss: 826.020023\n",
      "Epoch 33, loss: 791.399738\n",
      "Epoch 34, loss: 776.522978\n",
      "Epoch 35, loss: 764.511046\n",
      "Epoch 36, loss: 737.999176\n",
      "Epoch 37, loss: 804.153784\n",
      "Epoch 38, loss: 914.500213\n",
      "Epoch 39, loss: 805.689420\n",
      "Epoch 40, loss: 877.660332\n",
      "Epoch 41, loss: 1034.299019\n",
      "Epoch 42, loss: 1004.458841\n",
      "Epoch 43, loss: 915.984364\n",
      "Epoch 44, loss: 797.183929\n",
      "Epoch 45, loss: 922.239585\n",
      "Epoch 46, loss: 887.617884\n",
      "Epoch 47, loss: 892.504025\n",
      "Epoch 48, loss: 785.131577\n",
      "Epoch 49, loss: 872.463649\n",
      "Epoch 50, loss: 862.654967\n",
      "Epoch 51, loss: 901.048684\n",
      "Epoch 52, loss: 757.810859\n",
      "Epoch 53, loss: 821.465958\n",
      "Epoch 54, loss: 768.858769\n",
      "Epoch 55, loss: 711.341967\n",
      "Epoch 56, loss: 681.524879\n",
      "Epoch 57, loss: 734.345671\n",
      "Epoch 58, loss: 755.871858\n",
      "Epoch 59, loss: 760.537322\n",
      "Epoch 60, loss: 810.195562\n",
      "Epoch 61, loss: 787.763007\n",
      "Epoch 62, loss: 843.127774\n",
      "Epoch 63, loss: 893.413805\n",
      "Epoch 64, loss: 757.564780\n",
      "Epoch 65, loss: 796.180870\n",
      "Epoch 66, loss: 933.390780\n",
      "Epoch 67, loss: 834.449655\n",
      "Epoch 68, loss: 936.556677\n",
      "Epoch 69, loss: 851.083734\n",
      "Epoch 70, loss: 889.467195\n",
      "Epoch 71, loss: 834.099824\n",
      "Epoch 72, loss: 955.251819\n",
      "Epoch 73, loss: 706.556712\n",
      "Epoch 74, loss: 714.443569\n",
      "Epoch 75, loss: 699.428151\n",
      "Epoch 76, loss: 781.668990\n",
      "Epoch 77, loss: 756.242147\n",
      "Epoch 78, loss: 758.359837\n",
      "Epoch 79, loss: 770.209425\n",
      "Epoch 80, loss: 704.530390\n",
      "Epoch 81, loss: 689.925924\n",
      "Epoch 82, loss: 712.382628\n",
      "Epoch 83, loss: 724.796479\n",
      "Epoch 84, loss: 794.640394\n",
      "Epoch 85, loss: 768.601288\n",
      "Epoch 86, loss: 832.650060\n",
      "Epoch 87, loss: 965.557005\n",
      "Epoch 88, loss: 940.972697\n",
      "Epoch 89, loss: 806.711730\n",
      "Epoch 90, loss: 894.324512\n",
      "Epoch 91, loss: 874.199765\n",
      "Epoch 92, loss: 805.404084\n",
      "Epoch 93, loss: 868.949116\n",
      "Epoch 94, loss: 941.148892\n",
      "Epoch 95, loss: 862.621767\n",
      "Epoch 96, loss: 738.475027\n",
      "Epoch 97, loss: 761.011115\n",
      "Epoch 98, loss: 692.009504\n",
      "Epoch 99, loss: 748.802569\n",
      "Epoch 100, loss: 743.481525\n",
      "Epoch 101, loss: 736.059888\n",
      "Epoch 102, loss: 780.795975\n",
      "Epoch 103, loss: 878.781238\n",
      "Epoch 104, loss: 906.278884\n",
      "Epoch 105, loss: 795.889940\n",
      "Epoch 106, loss: 803.269016\n",
      "Epoch 107, loss: 788.545930\n",
      "Epoch 108, loss: 813.712802\n",
      "Epoch 109, loss: 805.899527\n",
      "Epoch 110, loss: 939.090176\n",
      "Epoch 111, loss: 995.129016\n",
      "Epoch 112, loss: 833.856125\n",
      "Epoch 113, loss: 834.332068\n",
      "Epoch 114, loss: 829.119728\n",
      "Epoch 115, loss: 927.862468\n",
      "Epoch 116, loss: 1114.324465\n",
      "Epoch 117, loss: 872.810700\n",
      "Epoch 118, loss: 717.072219\n",
      "Epoch 119, loss: 827.238794\n",
      "Epoch 120, loss: 897.670753\n",
      "Epoch 121, loss: 766.803197\n",
      "Epoch 122, loss: 720.202287\n",
      "Epoch 123, loss: 731.155721\n",
      "Epoch 124, loss: 793.569714\n",
      "Epoch 125, loss: 753.622087\n",
      "Epoch 126, loss: 856.679470\n",
      "Epoch 127, loss: 778.496468\n",
      "Epoch 128, loss: 743.109434\n",
      "Epoch 129, loss: 746.061638\n",
      "Epoch 130, loss: 921.794163\n",
      "Epoch 131, loss: 844.750912\n",
      "Epoch 132, loss: 888.398365\n",
      "Epoch 133, loss: 793.336627\n",
      "Epoch 134, loss: 841.745111\n",
      "Epoch 135, loss: 968.766970\n",
      "Epoch 136, loss: 1066.518669\n",
      "Epoch 137, loss: 926.502924\n",
      "Epoch 138, loss: 876.145950\n",
      "Epoch 139, loss: 742.959010\n",
      "Epoch 140, loss: 739.297466\n",
      "Epoch 141, loss: 777.464450\n",
      "Epoch 142, loss: 824.960198\n",
      "Epoch 143, loss: 771.403823\n",
      "Epoch 144, loss: 755.187515\n",
      "Epoch 145, loss: 748.996202\n",
      "Epoch 146, loss: 759.530623\n",
      "Epoch 147, loss: 780.974064\n",
      "Epoch 148, loss: 691.252379\n",
      "Epoch 149, loss: 773.710482\n",
      "Epoch 150, loss: 822.495672\n",
      "Epoch 151, loss: 810.025507\n",
      "Epoch 152, loss: 677.054870\n",
      "Epoch 153, loss: 640.156871\n",
      "Epoch 154, loss: 673.091565\n",
      "Epoch 155, loss: 699.541454\n",
      "Epoch 156, loss: 724.091417\n",
      "Epoch 157, loss: 781.429175\n",
      "Epoch 158, loss: 894.086184\n",
      "Epoch 159, loss: 1054.247977\n",
      "Epoch 160, loss: 775.396974\n",
      "Epoch 161, loss: 817.768759\n",
      "Epoch 162, loss: 800.002503\n",
      "Epoch 163, loss: 738.279550\n",
      "Epoch 164, loss: 725.144228\n",
      "Epoch 165, loss: 805.374331\n",
      "Epoch 166, loss: 866.903388\n",
      "Epoch 167, loss: 874.099560\n",
      "Epoch 168, loss: 970.101304\n",
      "Epoch 169, loss: 1160.654189\n",
      "Epoch 170, loss: 839.537064\n",
      "Epoch 171, loss: 749.463831\n",
      "Epoch 172, loss: 769.312988\n",
      "Epoch 173, loss: 723.800350\n",
      "Epoch 174, loss: 712.671197\n",
      "Epoch 175, loss: 703.426290\n",
      "Epoch 176, loss: 704.477114\n",
      "Epoch 177, loss: 716.606961\n",
      "Epoch 178, loss: 700.367735\n",
      "Epoch 179, loss: 698.852362\n",
      "Epoch 180, loss: 740.673687\n",
      "Epoch 181, loss: 727.501211\n",
      "Epoch 182, loss: 747.310810\n",
      "Epoch 183, loss: 730.360173\n",
      "Epoch 184, loss: 689.149514\n",
      "Epoch 185, loss: 703.967841\n",
      "Epoch 186, loss: 749.974809\n",
      "Epoch 187, loss: 728.066560\n",
      "Epoch 188, loss: 708.839067\n",
      "Epoch 189, loss: 720.705781\n",
      "Epoch 190, loss: 819.889858\n",
      "Epoch 191, loss: 760.255146\n",
      "Epoch 192, loss: 797.381327\n",
      "Epoch 193, loss: 744.260212\n",
      "Epoch 194, loss: 713.101621\n",
      "Epoch 195, loss: 713.181162\n",
      "Epoch 196, loss: 713.705216\n",
      "Epoch 197, loss: 709.990220\n",
      "Epoch 198, loss: 782.485889\n",
      "Epoch 199, loss: 995.465340\n",
      "Epoch 0, loss: 690.927243\n",
      "Epoch 1, loss: 690.403401\n",
      "Epoch 2, loss: 689.924932\n",
      "Epoch 3, loss: 689.756484\n",
      "Epoch 4, loss: 689.981322\n",
      "Epoch 5, loss: 687.664439\n",
      "Epoch 6, loss: 691.504187\n",
      "Epoch 7, loss: 689.747048\n",
      "Epoch 8, loss: 690.153442\n",
      "Epoch 9, loss: 688.707899\n",
      "Epoch 10, loss: 687.451379\n",
      "Epoch 11, loss: 688.384797\n",
      "Epoch 12, loss: 690.126382\n",
      "Epoch 13, loss: 687.256062\n",
      "Epoch 14, loss: 686.203174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, loss: 684.843753\n",
      "Epoch 16, loss: 686.098998\n",
      "Epoch 17, loss: 684.149194\n",
      "Epoch 18, loss: 687.898496\n",
      "Epoch 19, loss: 685.033926\n",
      "Epoch 20, loss: 684.052563\n",
      "Epoch 21, loss: 689.808307\n",
      "Epoch 22, loss: 688.258003\n",
      "Epoch 23, loss: 687.366217\n",
      "Epoch 24, loss: 686.487117\n",
      "Epoch 25, loss: 684.655190\n",
      "Epoch 26, loss: 684.908529\n",
      "Epoch 27, loss: 686.396232\n",
      "Epoch 28, loss: 683.311065\n",
      "Epoch 29, loss: 685.663071\n",
      "Epoch 30, loss: 685.198996\n",
      "Epoch 31, loss: 683.297227\n",
      "Epoch 32, loss: 684.081846\n",
      "Epoch 33, loss: 686.282132\n",
      "Epoch 34, loss: 681.114005\n",
      "Epoch 35, loss: 683.138365\n",
      "Epoch 36, loss: 684.150371\n",
      "Epoch 37, loss: 685.256442\n",
      "Epoch 38, loss: 683.304053\n",
      "Epoch 39, loss: 680.992053\n",
      "Epoch 40, loss: 677.682077\n",
      "Epoch 41, loss: 680.142799\n",
      "Epoch 42, loss: 681.380325\n",
      "Epoch 43, loss: 680.000634\n",
      "Epoch 44, loss: 682.064486\n",
      "Epoch 45, loss: 679.604558\n",
      "Epoch 46, loss: 676.661525\n",
      "Epoch 47, loss: 682.007495\n",
      "Epoch 48, loss: 681.837590\n",
      "Epoch 49, loss: 680.923360\n",
      "Epoch 50, loss: 682.712099\n",
      "Epoch 51, loss: 679.196066\n",
      "Epoch 52, loss: 682.091454\n",
      "Epoch 53, loss: 678.392656\n",
      "Epoch 54, loss: 679.835677\n",
      "Epoch 55, loss: 680.711127\n",
      "Epoch 56, loss: 678.207868\n",
      "Epoch 57, loss: 675.875694\n",
      "Epoch 58, loss: 680.150586\n",
      "Epoch 59, loss: 678.428056\n",
      "Epoch 60, loss: 678.962210\n",
      "Epoch 61, loss: 675.723287\n",
      "Epoch 62, loss: 677.615165\n",
      "Epoch 63, loss: 677.091671\n",
      "Epoch 64, loss: 677.281472\n",
      "Epoch 65, loss: 681.377782\n",
      "Epoch 66, loss: 678.802119\n",
      "Epoch 67, loss: 673.669245\n",
      "Epoch 68, loss: 678.157894\n",
      "Epoch 69, loss: 674.887073\n",
      "Epoch 70, loss: 672.370501\n",
      "Epoch 71, loss: 672.032809\n",
      "Epoch 72, loss: 679.913729\n",
      "Epoch 73, loss: 668.575904\n",
      "Epoch 74, loss: 676.559027\n",
      "Epoch 75, loss: 679.792951\n",
      "Epoch 76, loss: 675.359206\n",
      "Epoch 77, loss: 679.771927\n",
      "Epoch 78, loss: 672.831715\n",
      "Epoch 79, loss: 677.150556\n",
      "Epoch 80, loss: 674.844362\n",
      "Epoch 81, loss: 673.633819\n",
      "Epoch 82, loss: 678.253470\n",
      "Epoch 83, loss: 679.124731\n",
      "Epoch 84, loss: 677.490660\n",
      "Epoch 85, loss: 676.397780\n",
      "Epoch 86, loss: 673.291937\n",
      "Epoch 87, loss: 676.376950\n",
      "Epoch 88, loss: 677.122861\n",
      "Epoch 89, loss: 673.250621\n",
      "Epoch 90, loss: 667.254270\n",
      "Epoch 91, loss: 676.861192\n",
      "Epoch 92, loss: 669.137660\n",
      "Epoch 93, loss: 668.390631\n",
      "Epoch 94, loss: 674.038130\n",
      "Epoch 95, loss: 671.481781\n",
      "Epoch 96, loss: 673.207645\n",
      "Epoch 97, loss: 674.667071\n",
      "Epoch 98, loss: 668.968721\n",
      "Epoch 99, loss: 676.152562\n",
      "Epoch 100, loss: 673.244584\n",
      "Epoch 101, loss: 672.705930\n",
      "Epoch 102, loss: 670.113272\n",
      "Epoch 103, loss: 668.735882\n",
      "Epoch 104, loss: 674.377888\n",
      "Epoch 105, loss: 669.648318\n",
      "Epoch 106, loss: 670.127180\n",
      "Epoch 107, loss: 666.414394\n",
      "Epoch 108, loss: 664.200734\n",
      "Epoch 109, loss: 672.915440\n",
      "Epoch 110, loss: 667.996079\n",
      "Epoch 111, loss: 675.674871\n",
      "Epoch 112, loss: 672.142316\n",
      "Epoch 113, loss: 669.310179\n",
      "Epoch 114, loss: 672.968470\n",
      "Epoch 115, loss: 662.460793\n",
      "Epoch 116, loss: 668.981411\n",
      "Epoch 117, loss: 672.243015\n",
      "Epoch 118, loss: 677.750981\n",
      "Epoch 119, loss: 671.099682\n",
      "Epoch 120, loss: 664.592565\n",
      "Epoch 121, loss: 665.555703\n",
      "Epoch 122, loss: 669.178851\n",
      "Epoch 123, loss: 668.699007\n",
      "Epoch 124, loss: 668.329866\n",
      "Epoch 125, loss: 670.518072\n",
      "Epoch 126, loss: 664.784287\n",
      "Epoch 127, loss: 666.326001\n",
      "Epoch 128, loss: 673.171736\n",
      "Epoch 129, loss: 667.986365\n",
      "Epoch 130, loss: 667.477724\n",
      "Epoch 131, loss: 673.101515\n",
      "Epoch 132, loss: 670.824618\n",
      "Epoch 133, loss: 668.070092\n",
      "Epoch 134, loss: 668.306095\n",
      "Epoch 135, loss: 664.415151\n",
      "Epoch 136, loss: 669.493052\n",
      "Epoch 137, loss: 670.426094\n",
      "Epoch 138, loss: 673.583399\n",
      "Epoch 139, loss: 657.697581\n",
      "Epoch 140, loss: 672.028822\n",
      "Epoch 141, loss: 667.938020\n",
      "Epoch 142, loss: 667.712408\n",
      "Epoch 143, loss: 667.676140\n",
      "Epoch 144, loss: 667.829455\n",
      "Epoch 145, loss: 657.056562\n",
      "Epoch 146, loss: 669.766422\n",
      "Epoch 147, loss: 664.038949\n",
      "Epoch 148, loss: 665.700450\n",
      "Epoch 149, loss: 666.437126\n",
      "Epoch 150, loss: 667.152229\n",
      "Epoch 151, loss: 667.609488\n",
      "Epoch 152, loss: 664.162639\n",
      "Epoch 153, loss: 662.432206\n",
      "Epoch 154, loss: 659.828329\n",
      "Epoch 155, loss: 664.859537\n",
      "Epoch 156, loss: 664.909827\n",
      "Epoch 157, loss: 665.693916\n",
      "Epoch 158, loss: 672.047362\n",
      "Epoch 159, loss: 663.694706\n",
      "Epoch 160, loss: 669.232042\n",
      "Epoch 161, loss: 666.805534\n",
      "Epoch 162, loss: 661.452905\n",
      "Epoch 163, loss: 656.976156\n",
      "Epoch 164, loss: 658.586504\n",
      "Epoch 165, loss: 657.532030\n",
      "Epoch 166, loss: 656.855646\n",
      "Epoch 167, loss: 662.851045\n",
      "Epoch 168, loss: 655.796039\n",
      "Epoch 169, loss: 660.947199\n",
      "Epoch 170, loss: 657.313276\n",
      "Epoch 171, loss: 658.088807\n",
      "Epoch 172, loss: 669.891972\n",
      "Epoch 173, loss: 663.426247\n",
      "Epoch 174, loss: 657.571716\n",
      "Epoch 175, loss: 662.816661\n",
      "Epoch 176, loss: 665.984148\n",
      "Epoch 177, loss: 661.951913\n",
      "Epoch 178, loss: 671.102620\n",
      "Epoch 179, loss: 656.048718\n",
      "Epoch 180, loss: 655.807658\n",
      "Epoch 181, loss: 662.552495\n",
      "Epoch 182, loss: 672.588670\n",
      "Epoch 183, loss: 664.273647\n",
      "Epoch 184, loss: 664.761507\n",
      "Epoch 185, loss: 654.660237\n",
      "Epoch 186, loss: 668.848500\n",
      "Epoch 187, loss: 659.836050\n",
      "Epoch 188, loss: 665.221279\n",
      "Epoch 189, loss: 668.357941\n",
      "Epoch 190, loss: 661.025075\n",
      "Epoch 191, loss: 669.509779\n",
      "Epoch 192, loss: 663.407276\n",
      "Epoch 193, loss: 669.989834\n",
      "Epoch 194, loss: 655.037196\n",
      "Epoch 195, loss: 658.765410\n",
      "Epoch 196, loss: 659.864586\n",
      "Epoch 197, loss: 654.048857\n",
      "Epoch 198, loss: 665.873035\n",
      "Epoch 199, loss: 658.967452\n",
      "Epoch 0, loss: 659.983819\n",
      "Epoch 1, loss: 653.548481\n",
      "Epoch 2, loss: 660.115100\n",
      "Epoch 3, loss: 662.252853\n",
      "Epoch 4, loss: 657.871391\n",
      "Epoch 5, loss: 660.345909\n",
      "Epoch 6, loss: 659.607779\n",
      "Epoch 7, loss: 671.541075\n",
      "Epoch 8, loss: 657.411502\n",
      "Epoch 9, loss: 659.372575\n",
      "Epoch 10, loss: 664.800181\n",
      "Epoch 11, loss: 657.255589\n",
      "Epoch 12, loss: 665.246787\n",
      "Epoch 13, loss: 657.626339\n",
      "Epoch 14, loss: 665.253089\n",
      "Epoch 15, loss: 656.643196\n",
      "Epoch 16, loss: 657.413809\n",
      "Epoch 17, loss: 663.183762\n",
      "Epoch 18, loss: 661.832607\n",
      "Epoch 19, loss: 670.722485\n",
      "Epoch 20, loss: 644.584413\n",
      "Epoch 21, loss: 656.841305\n",
      "Epoch 22, loss: 666.888009\n",
      "Epoch 23, loss: 655.579964\n",
      "Epoch 24, loss: 653.913925\n",
      "Epoch 25, loss: 647.656709\n",
      "Epoch 26, loss: 657.457079\n",
      "Epoch 27, loss: 662.216977\n",
      "Epoch 28, loss: 650.118346\n",
      "Epoch 29, loss: 656.714614\n",
      "Epoch 30, loss: 661.040381\n",
      "Epoch 31, loss: 656.277150\n",
      "Epoch 32, loss: 652.220933\n",
      "Epoch 33, loss: 658.196317\n",
      "Epoch 34, loss: 659.241381\n",
      "Epoch 35, loss: 662.021937\n",
      "Epoch 36, loss: 659.254444\n",
      "Epoch 37, loss: 663.108940\n",
      "Epoch 38, loss: 658.078488\n",
      "Epoch 39, loss: 659.973475\n",
      "Epoch 40, loss: 663.592258\n",
      "Epoch 41, loss: 658.163994\n",
      "Epoch 42, loss: 656.539101\n",
      "Epoch 43, loss: 657.436233\n",
      "Epoch 44, loss: 648.634481\n",
      "Epoch 45, loss: 645.779214\n",
      "Epoch 46, loss: 655.208101\n",
      "Epoch 47, loss: 657.412239\n",
      "Epoch 48, loss: 654.639931\n",
      "Epoch 49, loss: 655.969016\n",
      "Epoch 50, loss: 646.620613\n",
      "Epoch 51, loss: 658.302873\n",
      "Epoch 52, loss: 658.065824\n",
      "Epoch 53, loss: 657.053830\n",
      "Epoch 54, loss: 657.565992\n",
      "Epoch 55, loss: 659.887611\n",
      "Epoch 56, loss: 654.815205\n",
      "Epoch 57, loss: 651.552558\n",
      "Epoch 58, loss: 654.467414\n",
      "Epoch 59, loss: 653.836766\n",
      "Epoch 60, loss: 656.065549\n",
      "Epoch 61, loss: 657.617283\n",
      "Epoch 62, loss: 655.098861\n",
      "Epoch 63, loss: 657.181833\n",
      "Epoch 64, loss: 660.077330\n",
      "Epoch 65, loss: 657.920836\n",
      "Epoch 66, loss: 658.271397\n",
      "Epoch 67, loss: 655.763343\n",
      "Epoch 68, loss: 646.588539\n",
      "Epoch 69, loss: 651.678143\n",
      "Epoch 70, loss: 660.638683\n",
      "Epoch 71, loss: 652.867102\n",
      "Epoch 72, loss: 648.009743\n",
      "Epoch 73, loss: 651.480327\n",
      "Epoch 74, loss: 654.331514\n",
      "Epoch 75, loss: 656.490670\n",
      "Epoch 76, loss: 663.519993\n",
      "Epoch 77, loss: 663.482740\n",
      "Epoch 78, loss: 643.097602\n",
      "Epoch 79, loss: 661.299562\n",
      "Epoch 80, loss: 667.420069\n",
      "Epoch 81, loss: 649.018558\n",
      "Epoch 82, loss: 655.685826\n",
      "Epoch 83, loss: 660.046320\n",
      "Epoch 84, loss: 656.659730\n",
      "Epoch 85, loss: 658.656601\n",
      "Epoch 86, loss: 655.168704\n",
      "Epoch 87, loss: 663.947745\n",
      "Epoch 88, loss: 646.658861\n",
      "Epoch 89, loss: 651.090139\n",
      "Epoch 90, loss: 657.759683\n",
      "Epoch 91, loss: 647.504420\n",
      "Epoch 92, loss: 665.882584\n",
      "Epoch 93, loss: 652.084349\n",
      "Epoch 94, loss: 657.584809\n",
      "Epoch 95, loss: 640.422230\n",
      "Epoch 96, loss: 650.422003\n",
      "Epoch 97, loss: 657.833633\n",
      "Epoch 98, loss: 659.353583\n",
      "Epoch 99, loss: 658.298505\n",
      "Epoch 100, loss: 649.207075\n",
      "Epoch 101, loss: 653.781517\n",
      "Epoch 102, loss: 656.712005\n",
      "Epoch 103, loss: 663.170167\n",
      "Epoch 104, loss: 647.052907\n",
      "Epoch 105, loss: 660.546511\n",
      "Epoch 106, loss: 648.988731\n",
      "Epoch 107, loss: 657.211674\n",
      "Epoch 108, loss: 652.434659\n",
      "Epoch 109, loss: 651.110877\n",
      "Epoch 110, loss: 641.363009\n",
      "Epoch 111, loss: 657.128593\n",
      "Epoch 112, loss: 663.053307\n",
      "Epoch 113, loss: 657.294051\n",
      "Epoch 114, loss: 653.704028\n",
      "Epoch 115, loss: 654.735466\n",
      "Epoch 116, loss: 653.258168\n",
      "Epoch 117, loss: 654.819353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, loss: 656.669507\n",
      "Epoch 119, loss: 645.500783\n",
      "Epoch 120, loss: 650.996431\n",
      "Epoch 121, loss: 658.492108\n",
      "Epoch 122, loss: 640.387413\n",
      "Epoch 123, loss: 655.171862\n",
      "Epoch 124, loss: 652.528817\n",
      "Epoch 125, loss: 650.875451\n",
      "Epoch 126, loss: 640.347783\n",
      "Epoch 127, loss: 648.006202\n",
      "Epoch 128, loss: 653.390106\n",
      "Epoch 129, loss: 649.668066\n",
      "Epoch 130, loss: 651.958366\n",
      "Epoch 131, loss: 661.939592\n",
      "Epoch 132, loss: 651.909596\n",
      "Epoch 133, loss: 654.644198\n",
      "Epoch 134, loss: 655.786025\n",
      "Epoch 135, loss: 660.750075\n",
      "Epoch 136, loss: 641.709360\n",
      "Epoch 137, loss: 653.187099\n",
      "Epoch 138, loss: 656.342939\n",
      "Epoch 139, loss: 646.660299\n",
      "Epoch 140, loss: 654.636211\n",
      "Epoch 141, loss: 653.110557\n",
      "Epoch 142, loss: 659.168436\n",
      "Epoch 143, loss: 655.184698\n",
      "Epoch 144, loss: 658.731947\n",
      "Epoch 145, loss: 655.207197\n",
      "Epoch 146, loss: 645.576900\n",
      "Epoch 147, loss: 645.608717\n",
      "Epoch 148, loss: 647.465244\n",
      "Epoch 149, loss: 634.237116\n",
      "Epoch 150, loss: 651.507572\n",
      "Epoch 151, loss: 649.478951\n",
      "Epoch 152, loss: 646.265464\n",
      "Epoch 153, loss: 646.927222\n",
      "Epoch 154, loss: 664.221307\n",
      "Epoch 155, loss: 644.425255\n",
      "Epoch 156, loss: 653.620935\n",
      "Epoch 157, loss: 657.310678\n",
      "Epoch 158, loss: 648.400845\n",
      "Epoch 159, loss: 657.559181\n",
      "Epoch 160, loss: 648.607768\n",
      "Epoch 161, loss: 638.507641\n",
      "Epoch 162, loss: 657.622456\n",
      "Epoch 163, loss: 652.323056\n",
      "Epoch 164, loss: 645.685153\n",
      "Epoch 165, loss: 662.233436\n",
      "Epoch 166, loss: 653.324919\n",
      "Epoch 167, loss: 650.470236\n",
      "Epoch 168, loss: 651.031449\n",
      "Epoch 169, loss: 642.547636\n",
      "Epoch 170, loss: 667.055643\n",
      "Epoch 171, loss: 655.263818\n",
      "Epoch 172, loss: 652.892368\n",
      "Epoch 173, loss: 649.075438\n",
      "Epoch 174, loss: 646.372569\n",
      "Epoch 175, loss: 647.329652\n",
      "Epoch 176, loss: 658.212799\n",
      "Epoch 177, loss: 652.232170\n",
      "Epoch 178, loss: 653.193143\n",
      "Epoch 179, loss: 658.664813\n",
      "Epoch 180, loss: 653.716821\n",
      "Epoch 181, loss: 656.304510\n",
      "Epoch 182, loss: 652.903992\n",
      "Epoch 183, loss: 647.263559\n",
      "Epoch 184, loss: 640.315424\n",
      "Epoch 185, loss: 647.853243\n",
      "Epoch 186, loss: 641.023192\n",
      "Epoch 187, loss: 648.388182\n",
      "Epoch 188, loss: 650.527959\n",
      "Epoch 189, loss: 651.790031\n",
      "Epoch 190, loss: 646.744187\n",
      "Epoch 191, loss: 662.528942\n",
      "Epoch 192, loss: 640.924848\n",
      "Epoch 193, loss: 634.858258\n",
      "Epoch 194, loss: 651.543508\n",
      "Epoch 195, loss: 651.733645\n",
      "Epoch 196, loss: 647.638219\n",
      "Epoch 197, loss: 639.688577\n",
      "Epoch 198, loss: 652.571126\n",
      "Epoch 199, loss: 651.216373\n",
      "Epoch 0, loss: 655.207838\n",
      "Epoch 1, loss: 648.499853\n",
      "Epoch 2, loss: 643.682285\n",
      "Epoch 3, loss: 650.202297\n",
      "Epoch 4, loss: 658.556833\n",
      "Epoch 5, loss: 654.101566\n",
      "Epoch 6, loss: 663.906549\n",
      "Epoch 7, loss: 652.964258\n",
      "Epoch 8, loss: 653.144888\n",
      "Epoch 9, loss: 651.297239\n",
      "Epoch 10, loss: 652.144913\n",
      "Epoch 11, loss: 656.180485\n",
      "Epoch 12, loss: 655.901760\n",
      "Epoch 13, loss: 651.988983\n",
      "Epoch 14, loss: 651.846813\n",
      "Epoch 15, loss: 662.368893\n",
      "Epoch 16, loss: 633.516483\n",
      "Epoch 17, loss: 652.639125\n",
      "Epoch 18, loss: 661.626968\n",
      "Epoch 19, loss: 639.560081\n",
      "Epoch 20, loss: 656.183228\n",
      "Epoch 21, loss: 650.074947\n",
      "Epoch 22, loss: 654.560168\n",
      "Epoch 23, loss: 661.138176\n",
      "Epoch 24, loss: 646.251478\n",
      "Epoch 25, loss: 636.817614\n",
      "Epoch 26, loss: 658.865304\n",
      "Epoch 27, loss: 644.945575\n",
      "Epoch 28, loss: 653.927533\n",
      "Epoch 29, loss: 655.533390\n",
      "Epoch 30, loss: 652.711002\n",
      "Epoch 31, loss: 663.326226\n",
      "Epoch 32, loss: 652.304683\n",
      "Epoch 33, loss: 643.334257\n",
      "Epoch 34, loss: 648.429949\n",
      "Epoch 35, loss: 634.767326\n",
      "Epoch 36, loss: 649.552058\n",
      "Epoch 37, loss: 648.934910\n",
      "Epoch 38, loss: 633.494964\n",
      "Epoch 39, loss: 644.103175\n",
      "Epoch 40, loss: 644.738769\n",
      "Epoch 41, loss: 651.945256\n",
      "Epoch 42, loss: 640.016531\n",
      "Epoch 43, loss: 645.195511\n",
      "Epoch 44, loss: 654.412385\n",
      "Epoch 45, loss: 645.633949\n",
      "Epoch 46, loss: 643.181043\n",
      "Epoch 47, loss: 660.277187\n",
      "Epoch 48, loss: 649.471853\n",
      "Epoch 49, loss: 650.295899\n",
      "Epoch 50, loss: 650.854826\n",
      "Epoch 51, loss: 640.858529\n",
      "Epoch 52, loss: 655.707586\n",
      "Epoch 53, loss: 640.229448\n",
      "Epoch 54, loss: 641.039537\n",
      "Epoch 55, loss: 648.004346\n",
      "Epoch 56, loss: 653.836174\n",
      "Epoch 57, loss: 647.673312\n",
      "Epoch 58, loss: 656.596312\n",
      "Epoch 59, loss: 654.464757\n",
      "Epoch 60, loss: 641.052683\n",
      "Epoch 61, loss: 651.311374\n",
      "Epoch 62, loss: 643.031992\n",
      "Epoch 63, loss: 641.404007\n",
      "Epoch 64, loss: 655.544597\n",
      "Epoch 65, loss: 648.936922\n",
      "Epoch 66, loss: 650.202967\n",
      "Epoch 67, loss: 647.106498\n",
      "Epoch 68, loss: 658.104528\n",
      "Epoch 69, loss: 643.895431\n",
      "Epoch 70, loss: 642.177665\n",
      "Epoch 71, loss: 667.288991\n",
      "Epoch 72, loss: 637.380123\n",
      "Epoch 73, loss: 653.824728\n",
      "Epoch 74, loss: 648.472820\n",
      "Epoch 75, loss: 640.723249\n",
      "Epoch 76, loss: 632.562595\n",
      "Epoch 77, loss: 645.700078\n",
      "Epoch 78, loss: 667.798347\n",
      "Epoch 79, loss: 652.356078\n",
      "Epoch 80, loss: 641.560296\n",
      "Epoch 81, loss: 645.965028\n",
      "Epoch 82, loss: 639.254966\n",
      "Epoch 83, loss: 647.444526\n",
      "Epoch 84, loss: 639.268550\n",
      "Epoch 85, loss: 645.734289\n",
      "Epoch 86, loss: 654.361368\n",
      "Epoch 87, loss: 653.791521\n",
      "Epoch 88, loss: 656.020716\n",
      "Epoch 89, loss: 652.554562\n",
      "Epoch 90, loss: 635.430903\n",
      "Epoch 91, loss: 652.521498\n",
      "Epoch 92, loss: 653.337855\n",
      "Epoch 93, loss: 646.722145\n",
      "Epoch 94, loss: 656.906584\n",
      "Epoch 95, loss: 645.626013\n",
      "Epoch 96, loss: 645.530696\n",
      "Epoch 97, loss: 650.093705\n",
      "Epoch 98, loss: 647.980313\n",
      "Epoch 99, loss: 655.384765\n",
      "Epoch 100, loss: 662.277167\n",
      "Epoch 101, loss: 641.741052\n",
      "Epoch 102, loss: 653.092889\n",
      "Epoch 103, loss: 647.871408\n",
      "Epoch 104, loss: 648.424741\n",
      "Epoch 105, loss: 641.480804\n",
      "Epoch 106, loss: 658.049447\n",
      "Epoch 107, loss: 653.071840\n",
      "Epoch 108, loss: 639.857146\n",
      "Epoch 109, loss: 638.565693\n",
      "Epoch 110, loss: 642.742164\n",
      "Epoch 111, loss: 642.869380\n",
      "Epoch 112, loss: 644.452420\n",
      "Epoch 113, loss: 651.382072\n",
      "Epoch 114, loss: 640.312134\n",
      "Epoch 115, loss: 651.825230\n",
      "Epoch 116, loss: 640.099085\n",
      "Epoch 117, loss: 650.208057\n",
      "Epoch 118, loss: 641.187696\n",
      "Epoch 119, loss: 654.427761\n",
      "Epoch 120, loss: 645.470004\n",
      "Epoch 121, loss: 657.888840\n",
      "Epoch 122, loss: 654.194326\n",
      "Epoch 123, loss: 646.383171\n",
      "Epoch 124, loss: 646.365517\n",
      "Epoch 125, loss: 636.603260\n",
      "Epoch 126, loss: 645.089004\n",
      "Epoch 127, loss: 643.436779\n",
      "Epoch 128, loss: 658.846245\n",
      "Epoch 129, loss: 651.271454\n",
      "Epoch 130, loss: 648.292249\n",
      "Epoch 131, loss: 645.980887\n",
      "Epoch 132, loss: 654.017986\n",
      "Epoch 133, loss: 656.224543\n",
      "Epoch 134, loss: 641.129193\n",
      "Epoch 135, loss: 644.616972\n",
      "Epoch 136, loss: 650.263823\n",
      "Epoch 137, loss: 653.045773\n",
      "Epoch 138, loss: 649.008478\n",
      "Epoch 139, loss: 635.023957\n",
      "Epoch 140, loss: 647.517206\n",
      "Epoch 141, loss: 647.287121\n",
      "Epoch 142, loss: 644.280945\n",
      "Epoch 143, loss: 653.291973\n",
      "Epoch 144, loss: 647.253328\n",
      "Epoch 145, loss: 659.855418\n",
      "Epoch 146, loss: 657.356143\n",
      "Epoch 147, loss: 635.954201\n",
      "Epoch 148, loss: 646.712094\n",
      "Epoch 149, loss: 643.737811\n",
      "Epoch 150, loss: 644.176597\n",
      "Epoch 151, loss: 638.695029\n",
      "Epoch 152, loss: 638.355269\n",
      "Epoch 153, loss: 640.997942\n",
      "Epoch 154, loss: 644.120663\n",
      "Epoch 155, loss: 636.598417\n",
      "Epoch 156, loss: 641.492826\n",
      "Epoch 157, loss: 645.172997\n",
      "Epoch 158, loss: 641.281541\n",
      "Epoch 159, loss: 648.446377\n",
      "Epoch 160, loss: 649.964606\n",
      "Epoch 161, loss: 644.318486\n",
      "Epoch 162, loss: 642.988504\n",
      "Epoch 163, loss: 648.848801\n",
      "Epoch 164, loss: 648.163131\n",
      "Epoch 165, loss: 643.956486\n",
      "Epoch 166, loss: 638.754389\n",
      "Epoch 167, loss: 648.247977\n",
      "Epoch 168, loss: 656.906641\n",
      "Epoch 169, loss: 651.812795\n",
      "Epoch 170, loss: 647.020375\n",
      "Epoch 171, loss: 641.375851\n",
      "Epoch 172, loss: 647.910898\n",
      "Epoch 173, loss: 639.607198\n",
      "Epoch 174, loss: 650.545743\n",
      "Epoch 175, loss: 634.710732\n",
      "Epoch 176, loss: 645.194179\n",
      "Epoch 177, loss: 632.926583\n",
      "Epoch 178, loss: 643.061611\n",
      "Epoch 179, loss: 645.913493\n",
      "Epoch 180, loss: 639.913927\n",
      "Epoch 181, loss: 653.274106\n",
      "Epoch 182, loss: 641.379401\n",
      "Epoch 183, loss: 655.589009\n",
      "Epoch 184, loss: 640.518310\n",
      "Epoch 185, loss: 645.846511\n",
      "Epoch 186, loss: 647.843057\n",
      "Epoch 187, loss: 647.917122\n",
      "Epoch 188, loss: 650.180557\n",
      "Epoch 189, loss: 650.327493\n",
      "Epoch 190, loss: 631.991817\n",
      "Epoch 191, loss: 662.446361\n",
      "Epoch 192, loss: 645.146027\n",
      "Epoch 193, loss: 644.111724\n",
      "Epoch 194, loss: 643.469090\n",
      "Epoch 195, loss: 640.715210\n",
      "Epoch 196, loss: 636.098597\n",
      "Epoch 197, loss: 643.684767\n",
      "Epoch 198, loss: 649.937758\n",
      "Epoch 199, loss: 639.088381\n",
      "Epoch 0, loss: 690.918957\n",
      "Epoch 1, loss: 691.038284\n",
      "Epoch 2, loss: 690.555879\n",
      "Epoch 3, loss: 690.614313\n",
      "Epoch 4, loss: 690.614390\n",
      "Epoch 5, loss: 690.691897\n",
      "Epoch 6, loss: 690.162744\n",
      "Epoch 7, loss: 690.649351\n",
      "Epoch 8, loss: 690.324406\n",
      "Epoch 9, loss: 690.398238\n",
      "Epoch 10, loss: 690.827799\n",
      "Epoch 11, loss: 690.295448\n",
      "Epoch 12, loss: 690.411771\n",
      "Epoch 13, loss: 690.276715\n",
      "Epoch 14, loss: 690.314186\n",
      "Epoch 15, loss: 690.378093\n",
      "Epoch 16, loss: 690.846635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, loss: 690.528638\n",
      "Epoch 18, loss: 690.090798\n",
      "Epoch 19, loss: 689.919772\n",
      "Epoch 20, loss: 691.214169\n",
      "Epoch 21, loss: 690.007092\n",
      "Epoch 22, loss: 690.394051\n",
      "Epoch 23, loss: 689.767391\n",
      "Epoch 24, loss: 690.614119\n",
      "Epoch 25, loss: 690.036958\n",
      "Epoch 26, loss: 690.055921\n",
      "Epoch 27, loss: 689.882175\n",
      "Epoch 28, loss: 689.880745\n",
      "Epoch 29, loss: 689.587239\n",
      "Epoch 30, loss: 689.803897\n",
      "Epoch 31, loss: 690.216083\n",
      "Epoch 32, loss: 689.831058\n",
      "Epoch 33, loss: 690.404468\n",
      "Epoch 34, loss: 690.037343\n",
      "Epoch 35, loss: 689.802710\n",
      "Epoch 36, loss: 690.271749\n",
      "Epoch 37, loss: 689.350857\n",
      "Epoch 38, loss: 689.797142\n",
      "Epoch 39, loss: 690.329832\n",
      "Epoch 40, loss: 689.811020\n",
      "Epoch 41, loss: 689.808062\n",
      "Epoch 42, loss: 689.668748\n",
      "Epoch 43, loss: 689.405672\n",
      "Epoch 44, loss: 689.726002\n",
      "Epoch 45, loss: 689.913536\n",
      "Epoch 46, loss: 689.879739\n",
      "Epoch 47, loss: 689.722518\n",
      "Epoch 48, loss: 689.845075\n",
      "Epoch 49, loss: 690.145186\n",
      "Epoch 50, loss: 689.322708\n",
      "Epoch 51, loss: 689.598793\n",
      "Epoch 52, loss: 688.671524\n",
      "Epoch 53, loss: 688.962086\n",
      "Epoch 54, loss: 689.237521\n",
      "Epoch 55, loss: 688.743653\n",
      "Epoch 56, loss: 689.419922\n",
      "Epoch 57, loss: 688.863913\n",
      "Epoch 58, loss: 689.693347\n",
      "Epoch 59, loss: 689.135463\n",
      "Epoch 60, loss: 689.722429\n",
      "Epoch 61, loss: 689.444138\n",
      "Epoch 62, loss: 688.953563\n",
      "Epoch 63, loss: 688.798548\n",
      "Epoch 64, loss: 689.200112\n",
      "Epoch 65, loss: 689.694390\n",
      "Epoch 66, loss: 689.846513\n",
      "Epoch 67, loss: 689.415305\n",
      "Epoch 68, loss: 689.409824\n",
      "Epoch 69, loss: 688.476069\n",
      "Epoch 70, loss: 688.850831\n",
      "Epoch 71, loss: 688.399408\n",
      "Epoch 72, loss: 688.574706\n",
      "Epoch 73, loss: 688.569008\n",
      "Epoch 74, loss: 688.491895\n",
      "Epoch 75, loss: 688.795767\n",
      "Epoch 76, loss: 687.780529\n",
      "Epoch 77, loss: 689.348631\n",
      "Epoch 78, loss: 688.694981\n",
      "Epoch 79, loss: 688.445936\n",
      "Epoch 80, loss: 688.457936\n",
      "Epoch 81, loss: 688.755917\n",
      "Epoch 82, loss: 689.152009\n",
      "Epoch 83, loss: 689.673531\n",
      "Epoch 84, loss: 688.861050\n",
      "Epoch 85, loss: 687.918383\n",
      "Epoch 86, loss: 688.924781\n",
      "Epoch 87, loss: 688.165687\n",
      "Epoch 88, loss: 688.309133\n",
      "Epoch 89, loss: 688.096354\n",
      "Epoch 90, loss: 688.728241\n",
      "Epoch 91, loss: 690.207970\n",
      "Epoch 92, loss: 688.734759\n",
      "Epoch 93, loss: 687.719570\n",
      "Epoch 94, loss: 688.109288\n",
      "Epoch 95, loss: 688.346708\n",
      "Epoch 96, loss: 688.400797\n",
      "Epoch 97, loss: 687.304038\n",
      "Epoch 98, loss: 686.978484\n",
      "Epoch 99, loss: 688.484023\n",
      "Epoch 100, loss: 688.013038\n",
      "Epoch 101, loss: 687.868739\n",
      "Epoch 102, loss: 687.582529\n",
      "Epoch 103, loss: 689.286649\n",
      "Epoch 104, loss: 688.632683\n",
      "Epoch 105, loss: 688.676470\n",
      "Epoch 106, loss: 688.247546\n",
      "Epoch 107, loss: 687.391015\n",
      "Epoch 108, loss: 688.620713\n",
      "Epoch 109, loss: 688.091290\n",
      "Epoch 110, loss: 688.208158\n",
      "Epoch 111, loss: 687.663790\n",
      "Epoch 112, loss: 687.185081\n",
      "Epoch 113, loss: 687.658823\n",
      "Epoch 114, loss: 688.108784\n",
      "Epoch 115, loss: 687.216236\n",
      "Epoch 116, loss: 687.417811\n",
      "Epoch 117, loss: 687.315772\n",
      "Epoch 118, loss: 687.168181\n",
      "Epoch 119, loss: 687.475513\n",
      "Epoch 120, loss: 687.326439\n",
      "Epoch 121, loss: 687.309762\n",
      "Epoch 122, loss: 687.602976\n",
      "Epoch 123, loss: 686.603092\n",
      "Epoch 124, loss: 688.138852\n",
      "Epoch 125, loss: 686.671945\n",
      "Epoch 126, loss: 687.783270\n",
      "Epoch 127, loss: 686.506701\n",
      "Epoch 128, loss: 688.138417\n",
      "Epoch 129, loss: 687.709588\n",
      "Epoch 130, loss: 687.058583\n",
      "Epoch 131, loss: 687.467613\n",
      "Epoch 132, loss: 686.360287\n",
      "Epoch 133, loss: 686.909694\n",
      "Epoch 134, loss: 687.007792\n",
      "Epoch 135, loss: 687.623970\n",
      "Epoch 136, loss: 688.068877\n",
      "Epoch 137, loss: 687.266631\n",
      "Epoch 138, loss: 686.783111\n",
      "Epoch 139, loss: 686.531309\n",
      "Epoch 140, loss: 687.123798\n",
      "Epoch 141, loss: 687.110025\n",
      "Epoch 142, loss: 688.508750\n",
      "Epoch 143, loss: 687.434446\n",
      "Epoch 144, loss: 687.746184\n",
      "Epoch 145, loss: 686.935380\n",
      "Epoch 146, loss: 686.969374\n",
      "Epoch 147, loss: 688.110498\n",
      "Epoch 148, loss: 685.113889\n",
      "Epoch 149, loss: 686.923043\n",
      "Epoch 150, loss: 686.337344\n",
      "Epoch 151, loss: 686.097381\n",
      "Epoch 152, loss: 687.541762\n",
      "Epoch 153, loss: 687.124315\n",
      "Epoch 154, loss: 686.691421\n",
      "Epoch 155, loss: 686.481487\n",
      "Epoch 156, loss: 686.545663\n",
      "Epoch 157, loss: 686.082958\n",
      "Epoch 158, loss: 686.103691\n",
      "Epoch 159, loss: 686.771073\n",
      "Epoch 160, loss: 687.162171\n",
      "Epoch 161, loss: 686.811953\n",
      "Epoch 162, loss: 686.891530\n",
      "Epoch 163, loss: 685.197812\n",
      "Epoch 164, loss: 686.838462\n",
      "Epoch 165, loss: 687.004644\n",
      "Epoch 166, loss: 686.479873\n",
      "Epoch 167, loss: 685.601633\n",
      "Epoch 168, loss: 686.999133\n",
      "Epoch 169, loss: 685.677829\n",
      "Epoch 170, loss: 685.985800\n",
      "Epoch 171, loss: 686.751283\n",
      "Epoch 172, loss: 686.386523\n",
      "Epoch 173, loss: 685.801757\n",
      "Epoch 174, loss: 685.472564\n",
      "Epoch 175, loss: 686.982666\n",
      "Epoch 176, loss: 685.280435\n",
      "Epoch 177, loss: 687.418996\n",
      "Epoch 178, loss: 685.181768\n",
      "Epoch 179, loss: 686.586326\n",
      "Epoch 180, loss: 685.615006\n",
      "Epoch 181, loss: 685.124019\n",
      "Epoch 182, loss: 685.174606\n",
      "Epoch 183, loss: 687.025955\n",
      "Epoch 184, loss: 685.542698\n",
      "Epoch 185, loss: 685.040534\n",
      "Epoch 186, loss: 686.063720\n",
      "Epoch 187, loss: 685.622637\n",
      "Epoch 188, loss: 686.617494\n",
      "Epoch 189, loss: 685.411836\n",
      "Epoch 190, loss: 686.390983\n",
      "Epoch 191, loss: 686.678406\n",
      "Epoch 192, loss: 685.308918\n",
      "Epoch 193, loss: 686.749129\n",
      "Epoch 194, loss: 686.881657\n",
      "Epoch 195, loss: 686.245706\n",
      "Epoch 196, loss: 684.818547\n",
      "Epoch 197, loss: 685.577882\n",
      "Epoch 198, loss: 684.495188\n",
      "Epoch 199, loss: 685.334146\n",
      "Epoch 0, loss: 684.459026\n",
      "Epoch 1, loss: 684.845436\n",
      "Epoch 2, loss: 685.818414\n",
      "Epoch 3, loss: 685.898610\n",
      "Epoch 4, loss: 684.041787\n",
      "Epoch 5, loss: 685.827016\n",
      "Epoch 6, loss: 684.888403\n",
      "Epoch 7, loss: 684.539511\n",
      "Epoch 8, loss: 685.749859\n",
      "Epoch 9, loss: 683.982276\n",
      "Epoch 10, loss: 685.211180\n",
      "Epoch 11, loss: 685.219586\n",
      "Epoch 12, loss: 685.472488\n",
      "Epoch 13, loss: 685.311393\n",
      "Epoch 14, loss: 685.167513\n",
      "Epoch 15, loss: 685.400587\n",
      "Epoch 16, loss: 683.936347\n",
      "Epoch 17, loss: 683.514512\n",
      "Epoch 18, loss: 684.758545\n",
      "Epoch 19, loss: 685.378999\n",
      "Epoch 20, loss: 684.961471\n",
      "Epoch 21, loss: 684.732992\n",
      "Epoch 22, loss: 685.599953\n",
      "Epoch 23, loss: 685.292152\n",
      "Epoch 24, loss: 684.555571\n",
      "Epoch 25, loss: 684.670164\n",
      "Epoch 26, loss: 684.724294\n",
      "Epoch 27, loss: 685.123552\n",
      "Epoch 28, loss: 682.596298\n",
      "Epoch 29, loss: 683.814912\n",
      "Epoch 30, loss: 682.296600\n",
      "Epoch 31, loss: 685.318290\n",
      "Epoch 32, loss: 685.485405\n",
      "Epoch 33, loss: 685.410473\n",
      "Epoch 34, loss: 685.501499\n",
      "Epoch 35, loss: 685.052757\n",
      "Epoch 36, loss: 683.554459\n",
      "Epoch 37, loss: 687.124810\n",
      "Epoch 38, loss: 686.907587\n",
      "Epoch 39, loss: 683.424866\n",
      "Epoch 40, loss: 684.198839\n",
      "Epoch 41, loss: 685.205898\n",
      "Epoch 42, loss: 684.296475\n",
      "Epoch 43, loss: 684.561768\n",
      "Epoch 44, loss: 684.928570\n",
      "Epoch 45, loss: 686.316315\n",
      "Epoch 46, loss: 684.621182\n",
      "Epoch 47, loss: 685.950585\n",
      "Epoch 48, loss: 685.448588\n",
      "Epoch 49, loss: 684.903335\n",
      "Epoch 50, loss: 683.542505\n",
      "Epoch 51, loss: 682.934422\n",
      "Epoch 52, loss: 686.932664\n",
      "Epoch 53, loss: 684.708079\n",
      "Epoch 54, loss: 683.969163\n",
      "Epoch 55, loss: 686.298329\n",
      "Epoch 56, loss: 684.633916\n",
      "Epoch 57, loss: 685.367653\n",
      "Epoch 58, loss: 684.365484\n",
      "Epoch 59, loss: 686.330813\n",
      "Epoch 60, loss: 682.792460\n",
      "Epoch 61, loss: 684.861626\n",
      "Epoch 62, loss: 683.822589\n",
      "Epoch 63, loss: 684.256345\n",
      "Epoch 64, loss: 685.153395\n",
      "Epoch 65, loss: 682.258297\n",
      "Epoch 66, loss: 685.036028\n",
      "Epoch 67, loss: 682.088913\n",
      "Epoch 68, loss: 683.975823\n",
      "Epoch 69, loss: 685.656844\n",
      "Epoch 70, loss: 682.885393\n",
      "Epoch 71, loss: 685.202956\n",
      "Epoch 72, loss: 682.879632\n",
      "Epoch 73, loss: 683.483496\n",
      "Epoch 74, loss: 683.900648\n",
      "Epoch 75, loss: 685.020214\n",
      "Epoch 76, loss: 684.650902\n",
      "Epoch 77, loss: 683.689324\n",
      "Epoch 78, loss: 685.424005\n",
      "Epoch 79, loss: 683.472623\n",
      "Epoch 80, loss: 682.715152\n",
      "Epoch 81, loss: 682.824792\n",
      "Epoch 82, loss: 682.961954\n",
      "Epoch 83, loss: 684.787615\n",
      "Epoch 84, loss: 683.776214\n",
      "Epoch 85, loss: 684.609004\n",
      "Epoch 86, loss: 686.373976\n",
      "Epoch 87, loss: 684.417512\n",
      "Epoch 88, loss: 683.663586\n",
      "Epoch 89, loss: 683.799980\n",
      "Epoch 90, loss: 683.951567\n",
      "Epoch 91, loss: 682.221509\n",
      "Epoch 92, loss: 685.726689\n",
      "Epoch 93, loss: 683.663297\n",
      "Epoch 94, loss: 684.493763\n",
      "Epoch 95, loss: 684.232523\n",
      "Epoch 96, loss: 684.011050\n",
      "Epoch 97, loss: 683.535082\n",
      "Epoch 98, loss: 681.813109\n",
      "Epoch 99, loss: 685.027185\n",
      "Epoch 100, loss: 681.676634\n",
      "Epoch 101, loss: 682.826191\n",
      "Epoch 102, loss: 682.229047\n",
      "Epoch 103, loss: 682.911954\n",
      "Epoch 104, loss: 683.953445\n",
      "Epoch 105, loss: 681.963028\n",
      "Epoch 106, loss: 684.421188\n",
      "Epoch 107, loss: 684.062035\n",
      "Epoch 108, loss: 682.349954\n",
      "Epoch 109, loss: 682.369882\n",
      "Epoch 110, loss: 686.289317\n",
      "Epoch 111, loss: 682.490938\n",
      "Epoch 112, loss: 686.266574\n",
      "Epoch 113, loss: 681.708534\n",
      "Epoch 114, loss: 683.424735\n",
      "Epoch 115, loss: 684.567911\n",
      "Epoch 116, loss: 682.345611\n",
      "Epoch 117, loss: 683.785417\n",
      "Epoch 118, loss: 684.001064\n",
      "Epoch 119, loss: 681.662690\n",
      "Epoch 120, loss: 682.238134\n",
      "Epoch 121, loss: 682.239826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122, loss: 682.601234\n",
      "Epoch 123, loss: 683.797876\n",
      "Epoch 124, loss: 682.796150\n",
      "Epoch 125, loss: 685.070387\n",
      "Epoch 126, loss: 681.904131\n",
      "Epoch 127, loss: 684.489542\n",
      "Epoch 128, loss: 683.209355\n",
      "Epoch 129, loss: 682.761686\n",
      "Epoch 130, loss: 681.840842\n",
      "Epoch 131, loss: 683.967345\n",
      "Epoch 132, loss: 681.305231\n",
      "Epoch 133, loss: 681.505905\n",
      "Epoch 134, loss: 678.514407\n",
      "Epoch 135, loss: 683.437076\n",
      "Epoch 136, loss: 684.377887\n",
      "Epoch 137, loss: 683.053469\n",
      "Epoch 138, loss: 681.986811\n",
      "Epoch 139, loss: 681.659584\n",
      "Epoch 140, loss: 681.408918\n",
      "Epoch 141, loss: 680.975653\n",
      "Epoch 142, loss: 682.191118\n",
      "Epoch 143, loss: 682.480880\n",
      "Epoch 144, loss: 682.434960\n",
      "Epoch 145, loss: 681.474085\n",
      "Epoch 146, loss: 682.713979\n",
      "Epoch 147, loss: 683.201861\n",
      "Epoch 148, loss: 683.762229\n",
      "Epoch 149, loss: 681.924517\n",
      "Epoch 150, loss: 680.912914\n",
      "Epoch 151, loss: 679.805582\n",
      "Epoch 152, loss: 681.223159\n",
      "Epoch 153, loss: 681.327566\n",
      "Epoch 154, loss: 679.363871\n",
      "Epoch 155, loss: 683.354197\n",
      "Epoch 156, loss: 683.775205\n",
      "Epoch 157, loss: 684.761722\n",
      "Epoch 158, loss: 681.972889\n",
      "Epoch 159, loss: 682.610513\n",
      "Epoch 160, loss: 683.408248\n",
      "Epoch 161, loss: 680.289055\n",
      "Epoch 162, loss: 681.342037\n",
      "Epoch 163, loss: 681.118137\n",
      "Epoch 164, loss: 683.377512\n",
      "Epoch 165, loss: 682.500697\n",
      "Epoch 166, loss: 682.773993\n",
      "Epoch 167, loss: 684.348643\n",
      "Epoch 168, loss: 683.275594\n",
      "Epoch 169, loss: 681.426394\n",
      "Epoch 170, loss: 681.479026\n",
      "Epoch 171, loss: 681.330501\n",
      "Epoch 172, loss: 676.892201\n",
      "Epoch 173, loss: 682.602629\n",
      "Epoch 174, loss: 682.740040\n",
      "Epoch 175, loss: 681.685104\n",
      "Epoch 176, loss: 682.497941\n",
      "Epoch 177, loss: 680.393639\n",
      "Epoch 178, loss: 685.015739\n",
      "Epoch 179, loss: 682.287118\n",
      "Epoch 180, loss: 680.632394\n",
      "Epoch 181, loss: 684.010904\n",
      "Epoch 182, loss: 680.897581\n",
      "Epoch 183, loss: 680.020699\n",
      "Epoch 184, loss: 684.061585\n",
      "Epoch 185, loss: 679.296792\n",
      "Epoch 186, loss: 680.355865\n",
      "Epoch 187, loss: 682.326651\n",
      "Epoch 188, loss: 683.656572\n",
      "Epoch 189, loss: 680.470121\n",
      "Epoch 190, loss: 681.566182\n",
      "Epoch 191, loss: 682.427622\n",
      "Epoch 192, loss: 682.425590\n",
      "Epoch 193, loss: 683.620239\n",
      "Epoch 194, loss: 683.394426\n",
      "Epoch 195, loss: 681.927195\n",
      "Epoch 196, loss: 680.000529\n",
      "Epoch 197, loss: 683.582949\n",
      "Epoch 198, loss: 682.287682\n",
      "Epoch 199, loss: 679.666473\n",
      "Epoch 0, loss: 680.310870\n",
      "Epoch 1, loss: 681.856009\n",
      "Epoch 2, loss: 682.251410\n",
      "Epoch 3, loss: 681.396716\n",
      "Epoch 4, loss: 679.138005\n",
      "Epoch 5, loss: 682.855268\n",
      "Epoch 6, loss: 680.000861\n",
      "Epoch 7, loss: 680.268908\n",
      "Epoch 8, loss: 680.435111\n",
      "Epoch 9, loss: 680.242058\n",
      "Epoch 10, loss: 681.901801\n",
      "Epoch 11, loss: 680.625410\n",
      "Epoch 12, loss: 678.087597\n",
      "Epoch 13, loss: 681.425973\n",
      "Epoch 14, loss: 682.523350\n",
      "Epoch 15, loss: 679.665689\n",
      "Epoch 16, loss: 678.951058\n",
      "Epoch 17, loss: 682.697124\n",
      "Epoch 18, loss: 682.691372\n",
      "Epoch 19, loss: 680.942655\n",
      "Epoch 20, loss: 680.499126\n",
      "Epoch 21, loss: 681.444656\n",
      "Epoch 22, loss: 681.893340\n",
      "Epoch 23, loss: 684.101617\n",
      "Epoch 24, loss: 684.672853\n",
      "Epoch 25, loss: 682.360109\n",
      "Epoch 26, loss: 678.088037\n",
      "Epoch 27, loss: 683.033770\n",
      "Epoch 28, loss: 678.960616\n",
      "Epoch 29, loss: 682.512285\n",
      "Epoch 30, loss: 680.244093\n",
      "Epoch 31, loss: 680.558702\n",
      "Epoch 32, loss: 681.403064\n",
      "Epoch 33, loss: 680.275139\n",
      "Epoch 34, loss: 681.711682\n",
      "Epoch 35, loss: 679.807498\n",
      "Epoch 36, loss: 681.039345\n",
      "Epoch 37, loss: 679.141510\n",
      "Epoch 38, loss: 680.810357\n",
      "Epoch 39, loss: 681.261987\n",
      "Epoch 40, loss: 680.120757\n",
      "Epoch 41, loss: 679.895876\n",
      "Epoch 42, loss: 682.622553\n",
      "Epoch 43, loss: 678.742138\n",
      "Epoch 44, loss: 684.194405\n",
      "Epoch 45, loss: 681.707111\n",
      "Epoch 46, loss: 679.731711\n",
      "Epoch 47, loss: 677.747067\n",
      "Epoch 48, loss: 681.064031\n",
      "Epoch 49, loss: 678.853538\n",
      "Epoch 50, loss: 679.507270\n",
      "Epoch 51, loss: 679.938736\n",
      "Epoch 52, loss: 681.013423\n",
      "Epoch 53, loss: 677.182057\n",
      "Epoch 54, loss: 679.753761\n",
      "Epoch 55, loss: 681.223131\n",
      "Epoch 56, loss: 677.470543\n",
      "Epoch 57, loss: 677.625505\n",
      "Epoch 58, loss: 679.168981\n",
      "Epoch 59, loss: 682.230160\n",
      "Epoch 60, loss: 681.352862\n",
      "Epoch 61, loss: 682.075661\n",
      "Epoch 62, loss: 676.427871\n",
      "Epoch 63, loss: 680.176083\n",
      "Epoch 64, loss: 682.390916\n",
      "Epoch 65, loss: 676.350753\n",
      "Epoch 66, loss: 677.645312\n",
      "Epoch 67, loss: 678.381153\n",
      "Epoch 68, loss: 680.666250\n",
      "Epoch 69, loss: 683.722617\n",
      "Epoch 70, loss: 676.461418\n",
      "Epoch 71, loss: 678.382782\n",
      "Epoch 72, loss: 681.207424\n",
      "Epoch 73, loss: 679.159887\n",
      "Epoch 74, loss: 678.847828\n",
      "Epoch 75, loss: 676.680350\n",
      "Epoch 76, loss: 678.653526\n",
      "Epoch 77, loss: 682.335314\n",
      "Epoch 78, loss: 679.177994\n",
      "Epoch 79, loss: 682.932600\n",
      "Epoch 80, loss: 677.361293\n",
      "Epoch 81, loss: 680.259365\n",
      "Epoch 82, loss: 680.104513\n",
      "Epoch 83, loss: 679.245663\n",
      "Epoch 84, loss: 678.633232\n",
      "Epoch 85, loss: 679.366362\n",
      "Epoch 86, loss: 680.169876\n",
      "Epoch 87, loss: 679.673267\n",
      "Epoch 88, loss: 679.775371\n",
      "Epoch 89, loss: 681.698520\n",
      "Epoch 90, loss: 679.076712\n",
      "Epoch 91, loss: 677.789483\n",
      "Epoch 92, loss: 681.874784\n",
      "Epoch 93, loss: 680.403703\n",
      "Epoch 94, loss: 682.090334\n",
      "Epoch 95, loss: 682.196816\n",
      "Epoch 96, loss: 679.298272\n",
      "Epoch 97, loss: 682.561841\n",
      "Epoch 98, loss: 678.454041\n",
      "Epoch 99, loss: 682.543461\n",
      "Epoch 100, loss: 679.653970\n",
      "Epoch 101, loss: 680.541766\n",
      "Epoch 102, loss: 680.052891\n",
      "Epoch 103, loss: 675.653452\n",
      "Epoch 104, loss: 681.420789\n",
      "Epoch 105, loss: 678.050971\n",
      "Epoch 106, loss: 680.893022\n",
      "Epoch 107, loss: 679.190994\n",
      "Epoch 108, loss: 678.280679\n",
      "Epoch 109, loss: 678.132705\n",
      "Epoch 110, loss: 677.422334\n",
      "Epoch 111, loss: 680.840052\n",
      "Epoch 112, loss: 680.539716\n",
      "Epoch 113, loss: 678.126154\n",
      "Epoch 114, loss: 678.632149\n",
      "Epoch 115, loss: 679.653906\n",
      "Epoch 116, loss: 681.185759\n",
      "Epoch 117, loss: 677.226447\n",
      "Epoch 118, loss: 681.330673\n",
      "Epoch 119, loss: 678.086178\n",
      "Epoch 120, loss: 680.655141\n",
      "Epoch 121, loss: 678.233102\n",
      "Epoch 122, loss: 676.432463\n",
      "Epoch 123, loss: 676.054995\n",
      "Epoch 124, loss: 679.242883\n",
      "Epoch 125, loss: 680.210938\n",
      "Epoch 126, loss: 675.449117\n",
      "Epoch 127, loss: 681.264990\n",
      "Epoch 128, loss: 678.925887\n",
      "Epoch 129, loss: 679.826279\n",
      "Epoch 130, loss: 679.901744\n",
      "Epoch 131, loss: 678.944977\n",
      "Epoch 132, loss: 676.423172\n",
      "Epoch 133, loss: 682.503653\n",
      "Epoch 134, loss: 678.908184\n",
      "Epoch 135, loss: 679.912572\n",
      "Epoch 136, loss: 679.858771\n",
      "Epoch 137, loss: 678.866944\n",
      "Epoch 138, loss: 677.081822\n",
      "Epoch 139, loss: 682.708521\n",
      "Epoch 140, loss: 680.056433\n",
      "Epoch 141, loss: 676.815654\n",
      "Epoch 142, loss: 682.268359\n",
      "Epoch 143, loss: 676.159651\n",
      "Epoch 144, loss: 677.495585\n",
      "Epoch 145, loss: 676.635558\n",
      "Epoch 146, loss: 679.999148\n",
      "Epoch 147, loss: 677.390358\n",
      "Epoch 148, loss: 677.368358\n",
      "Epoch 149, loss: 679.469669\n",
      "Epoch 150, loss: 678.393694\n",
      "Epoch 151, loss: 679.632564\n",
      "Epoch 152, loss: 678.407317\n",
      "Epoch 153, loss: 681.212588\n",
      "Epoch 154, loss: 680.515818\n",
      "Epoch 155, loss: 677.981362\n",
      "Epoch 156, loss: 675.643359\n",
      "Epoch 157, loss: 678.593502\n",
      "Epoch 158, loss: 676.645588\n",
      "Epoch 159, loss: 681.097754\n",
      "Epoch 160, loss: 676.143666\n",
      "Epoch 161, loss: 678.328783\n",
      "Epoch 162, loss: 682.463300\n",
      "Epoch 163, loss: 679.307828\n",
      "Epoch 164, loss: 677.538387\n",
      "Epoch 165, loss: 676.188032\n",
      "Epoch 166, loss: 677.082034\n",
      "Epoch 167, loss: 677.135223\n",
      "Epoch 168, loss: 676.688714\n",
      "Epoch 169, loss: 677.809715\n",
      "Epoch 170, loss: 676.358536\n",
      "Epoch 171, loss: 678.215778\n",
      "Epoch 172, loss: 679.457355\n",
      "Epoch 173, loss: 677.096988\n",
      "Epoch 174, loss: 677.880267\n",
      "Epoch 175, loss: 677.025039\n",
      "Epoch 176, loss: 671.728856\n",
      "Epoch 177, loss: 678.305421\n",
      "Epoch 178, loss: 679.110761\n",
      "Epoch 179, loss: 678.159426\n",
      "Epoch 180, loss: 679.218593\n",
      "Epoch 181, loss: 678.080796\n",
      "Epoch 182, loss: 677.085346\n",
      "Epoch 183, loss: 675.435186\n",
      "Epoch 184, loss: 676.734552\n",
      "Epoch 185, loss: 680.860036\n",
      "Epoch 186, loss: 678.215593\n",
      "Epoch 187, loss: 676.138615\n",
      "Epoch 188, loss: 677.532301\n",
      "Epoch 189, loss: 677.583314\n",
      "Epoch 190, loss: 679.868102\n",
      "Epoch 191, loss: 674.702016\n",
      "Epoch 192, loss: 679.106842\n",
      "Epoch 193, loss: 678.082901\n",
      "Epoch 194, loss: 677.059873\n",
      "Epoch 195, loss: 673.364439\n",
      "Epoch 196, loss: 677.504526\n",
      "Epoch 197, loss: 679.807649\n",
      "Epoch 198, loss: 678.413092\n",
      "Epoch 199, loss: 673.362603\n",
      "best validation accuracy achieved: 0.240000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "for lr in learning_rates:\n",
    "    classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "    for reg in reg_strengths:\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs,\n",
    "                      learning_rate=lr, batch_size=batch_size, reg=reg)\n",
    "        prediction = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(prediction, val_y)\n",
    "        if not best_val_accuracy or accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = (lr, reg)\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
